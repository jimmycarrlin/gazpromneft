{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "\n",
    "from source.resnet import TorchModel, ResNet, BasicBlock, Bottleneck\n",
    "from source.callback import CompositeCallback, ClassificationReporter, Profiler, Saver, Tuner, DefaultCallback\n",
    "from source.plotting import matplotlib_imshow\n",
    "\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "cwd = Path(os.getcwd())\n",
    "\n",
    "\n",
    "train_dir = cwd / \"imagenette2-320\" / \"train\"\n",
    "\n",
    "tsfm_train = transforms.Compose([\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trainset = datasets.ImageFolder(root=train_dir, transform=tsfm_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-06-21 04:56:22 (running for 00:00:00.70)\n",
      "Memory usage on this node: 7.7/16.0 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/6.96 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/pavelkiselev/PycharmProjects/pythonProject4/ray_result/to_tune_2022-06-21_04-56-21\n",
      "Number of trials: 16/20 (15 PENDING, 1 RUNNING)\n",
      "+---------------------+----------+-----------------+--------------+----------+----------+----------+----------+-------------+\n",
      "| Trial name          | status   | loc             |   batch_size |   layer1 |   layer2 |   layer3 |   layer4 |          lr |\n",
      "|---------------------+----------+-----------------+--------------+----------+----------+----------+----------+-------------|\n",
      "| to_tune_59710_00000 | RUNNING  | 127.0.0.1:33743 |           32 |        3 |        2 |        6 |        2 | 0.000258306 |\n",
      "| to_tune_59710_00001 | PENDING  |                 |            4 |        3 |        3 |       15 |        3 | 0.00568554  |\n",
      "| to_tune_59710_00002 | PENDING  |                 |            4 |        3 |        3 |        6 |        3 | 0.000150412 |\n",
      "| to_tune_59710_00003 | PENDING  |                 |            8 |        2 |        2 |       14 |        3 | 0.00280319  |\n",
      "| to_tune_59710_00004 | PENDING  |                 |            4 |        2 |        3 |       12 |        2 | 0.00124253  |\n",
      "| to_tune_59710_00005 | PENDING  |                 |            4 |        2 |        3 |       12 |        2 | 0.000721661 |\n",
      "| to_tune_59710_00006 | PENDING  |                 |            2 |        3 |        2 |        7 |        2 | 0.0120684   |\n",
      "| to_tune_59710_00007 | PENDING  |                 |           32 |        2 |        3 |       15 |        3 | 0.000156297 |\n",
      "| to_tune_59710_00008 | PENDING  |                 |           16 |        3 |        2 |        6 |        2 | 0.0734653   |\n",
      "| to_tune_59710_00009 | PENDING  |                 |            4 |        2 |        3 |        5 |        3 | 0.0393444   |\n",
      "| to_tune_59710_00010 | PENDING  |                 |           32 |        3 |        2 |       13 |        2 | 0.000362885 |\n",
      "| to_tune_59710_00011 | PENDING  |                 |           64 |        2 |        3 |        4 |        2 | 0.0116825   |\n",
      "| to_tune_59710_00012 | PENDING  |                 |            4 |        2 |        2 |        6 |        3 | 0.00848958  |\n",
      "| to_tune_59710_00013 | PENDING  |                 |            2 |        3 |        2 |       11 |        3 | 0.00479028  |\n",
      "| to_tune_59710_00014 | PENDING  |                 |            2 |        3 |        3 |        6 |        3 | 0.000857253 |\n",
      "| to_tune_59710_00015 | PENDING  |                 |           64 |        2 |        2 |       11 |        2 | 0.000442054 |\n",
      "+---------------------+----------+-----------------+--------------+----------+----------+----------+----------+-------------+\n",
      "\n",
      "\n",
      "\u001B[2m\u001B[36m(func pid=33743)\u001B[0m Epoch loop:   0%|          | 0/100 [00:00<?, ?epoch/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(func pid=33743)\u001B[0m E0621 04:56:24.528817000 6158610432 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(func pid=33743)\u001B[0m Batch loop:   0%|          | 0/2 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(func pid=33753)\u001B[0m E0621 04:56:27.321573000 12901707776 fork_posix.cc:76]                 Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001B[2m\u001B[36m(func pid=33751)\u001B[0m E0621 04:56:27.306417000 6251950080 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001B[2m\u001B[36m(func pid=33754)\u001B[0m E0621 04:56:27.371380000 6265761792 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(func pid=33753)\u001B[0m Epoch loop:   0%|          | 0/100 [00:00<?, ?epoch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33753)\u001B[0m Batch loop:   0%|          | 0/2 [00:00<?, ?batch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33754)\u001B[0m Epoch loop:   0%|          | 0/100 [00:00<?, ?epoch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33754)\u001B[0m Batch loop:   0%|          | 0/2 [00:00<?, ?batch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33751)\u001B[0m Epoch loop:   0%|          | 0/100 [00:00<?, ?epoch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33751)\u001B[0m Batch loop:   0%|          | 0/2 [00:00<?, ?batch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33752)\u001B[0m Epoch loop:   0%|          | 0/100 [00:00<?, ?epoch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33752)\u001B[0m Batch loop:   0%|          | 0/2 [00:00<?, ?batch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33748)\u001B[0m Epoch loop:   0%|          | 0/100 [00:00<?, ?epoch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33748)\u001B[0m Batch loop:   0%|          | 0/2 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(func pid=33748)\u001B[0m E0621 04:56:27.523975000 6164344832 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001B[2m\u001B[36m(func pid=33752)\u001B[0m E0621 04:56:27.491097000 6220263424 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001B[2m\u001B[36m(func pid=33750)\u001B[0m E0621 04:56:27.562372000 6320353280 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "\u001B[2m\u001B[36m(func pid=33749)\u001B[0m E0621 04:56:27.589824000 6228832256 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(func pid=33750)\u001B[0m Epoch loop:   0%|          | 0/100 [00:00<?, ?epoch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33750)\u001B[0m Batch loop:   0%|          | 0/2 [00:00<?, ?batch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33749)\u001B[0m Epoch loop:   0%|          | 0/100 [00:00<?, ?epoch/s]\n",
      "\u001B[2m\u001B[36m(func pid=33749)\u001B[0m Batch loop:   0%|          | 0/2 [00:00<?, ?batch/s]\n",
      "== Status ==\n",
      "Current time: 2022-06-21 04:56:29 (running for 00:00:07.59)\n",
      "Memory usage on this node: 11.2/16.0 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 8.0/8 CPUs, 0/0 GPUs, 0.0/6.96 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/pavelkiselev/PycharmProjects/pythonProject4/ray_result/to_tune_2022-06-21_04-56-21\n",
      "Number of trials: 20/20 (12 PENDING, 8 RUNNING)\n",
      "+---------------------+----------+-----------------+--------------+----------+----------+----------+----------+-------------+\n",
      "| Trial name          | status   | loc             |   batch_size |   layer1 |   layer2 |   layer3 |   layer4 |          lr |\n",
      "|---------------------+----------+-----------------+--------------+----------+----------+----------+----------+-------------|\n",
      "| to_tune_59710_00000 | RUNNING  | 127.0.0.1:33743 |           32 |        3 |        2 |        6 |        2 | 0.000258306 |\n",
      "| to_tune_59710_00001 | RUNNING  | 127.0.0.1:33748 |            4 |        3 |        3 |       15 |        3 | 0.00568554  |\n",
      "| to_tune_59710_00002 | RUNNING  | 127.0.0.1:33749 |            4 |        3 |        3 |        6 |        3 | 0.000150412 |\n",
      "| to_tune_59710_00003 | RUNNING  | 127.0.0.1:33750 |            8 |        2 |        2 |       14 |        3 | 0.00280319  |\n",
      "| to_tune_59710_00004 | RUNNING  | 127.0.0.1:33751 |            4 |        2 |        3 |       12 |        2 | 0.00124253  |\n",
      "| to_tune_59710_00005 | RUNNING  | 127.0.0.1:33752 |            4 |        2 |        3 |       12 |        2 | 0.000721661 |\n",
      "| to_tune_59710_00006 | RUNNING  | 127.0.0.1:33753 |            2 |        3 |        2 |        7 |        2 | 0.0120684   |\n",
      "| to_tune_59710_00007 | RUNNING  | 127.0.0.1:33754 |           32 |        2 |        3 |       15 |        3 | 0.000156297 |\n",
      "| to_tune_59710_00008 | PENDING  |                 |           16 |        3 |        2 |        6 |        2 | 0.0734653   |\n",
      "| to_tune_59710_00009 | PENDING  |                 |            4 |        2 |        3 |        5 |        3 | 0.0393444   |\n",
      "| to_tune_59710_00010 | PENDING  |                 |           32 |        3 |        2 |       13 |        2 | 0.000362885 |\n",
      "| to_tune_59710_00011 | PENDING  |                 |           64 |        2 |        3 |        4 |        2 | 0.0116825   |\n",
      "| to_tune_59710_00012 | PENDING  |                 |            4 |        2 |        2 |        6 |        3 | 0.00848958  |\n",
      "| to_tune_59710_00013 | PENDING  |                 |            2 |        3 |        2 |       11 |        3 | 0.00479028  |\n",
      "| to_tune_59710_00014 | PENDING  |                 |            2 |        3 |        3 |        6 |        3 | 0.000857253 |\n",
      "| to_tune_59710_00015 | PENDING  |                 |           64 |        2 |        2 |       11 |        2 | 0.000442054 |\n",
      "| to_tune_59710_00016 | PENDING  |                 |           32 |        3 |        2 |       14 |        3 | 0.019874    |\n",
      "| to_tune_59710_00017 | PENDING  |                 |            4 |        2 |        3 |        8 |        2 | 0.00167345  |\n",
      "| to_tune_59710_00018 | PENDING  |                 |           32 |        2 |        2 |       15 |        2 | 0.0101252   |\n",
      "| to_tune_59710_00019 | PENDING  |                 |           16 |        2 |        3 |        6 |        3 | 0.00340739  |\n",
      "+---------------------+----------+-----------------+--------------+----------+----------+----------+----------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 04:56:32,981\tWARNING tune.py:682 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2022-06-21 04:56:32,997\tERROR trial_runner.py:886 -- Trial to_tune_59710_00006: Error processing event.\n",
      "NoneType: None\n",
      "\u001B[2m\u001B[36m(func pid=33753)\u001B[0m /opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001B[2m\u001B[36m(func pid=33753)\u001B[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for to_tune_59710_00006:\n",
      "  date: 2022-06-21_04-56-27\n",
      "  experiment_id: e75f63737bf04f9298d28fa5741fde6f\n",
      "  hostname: MacBook-Air-Pavel.local\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 33753\n",
      "  timestamp: 1655776587\n",
      "  trial_id: '59710_00006'\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-06-21 04:56:33 (running for 00:00:11.09)\n",
      "Memory usage on this node: 13.3/16.0 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 7.0/8 CPUs, 0/0 GPUs, 0.0/6.96 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/pavelkiselev/PycharmProjects/pythonProject4/ray_result/to_tune_2022-06-21_04-56-21\n",
      "Number of trials: 20/20 (1 ERROR, 12 PENDING, 7 RUNNING)\n",
      "+---------------------+----------+-----------------+--------------+----------+----------+----------+----------+-------------+\n",
      "| Trial name          | status   | loc             |   batch_size |   layer1 |   layer2 |   layer3 |   layer4 |          lr |\n",
      "|---------------------+----------+-----------------+--------------+----------+----------+----------+----------+-------------|\n",
      "| to_tune_59710_00000 | RUNNING  | 127.0.0.1:33743 |           32 |        3 |        2 |        6 |        2 | 0.000258306 |\n",
      "| to_tune_59710_00001 | RUNNING  | 127.0.0.1:33748 |            4 |        3 |        3 |       15 |        3 | 0.00568554  |\n",
      "| to_tune_59710_00002 | RUNNING  | 127.0.0.1:33749 |            4 |        3 |        3 |        6 |        3 | 0.000150412 |\n",
      "| to_tune_59710_00003 | RUNNING  | 127.0.0.1:33750 |            8 |        2 |        2 |       14 |        3 | 0.00280319  |\n",
      "| to_tune_59710_00004 | RUNNING  | 127.0.0.1:33751 |            4 |        2 |        3 |       12 |        2 | 0.00124253  |\n",
      "| to_tune_59710_00005 | RUNNING  | 127.0.0.1:33752 |            4 |        2 |        3 |       12 |        2 | 0.000721661 |\n",
      "| to_tune_59710_00007 | RUNNING  | 127.0.0.1:33754 |           32 |        2 |        3 |       15 |        3 | 0.000156297 |\n",
      "| to_tune_59710_00008 | PENDING  |                 |           16 |        3 |        2 |        6 |        2 | 0.0734653   |\n",
      "| to_tune_59710_00009 | PENDING  |                 |            4 |        2 |        3 |        5 |        3 | 0.0393444   |\n",
      "| to_tune_59710_00010 | PENDING  |                 |           32 |        3 |        2 |       13 |        2 | 0.000362885 |\n",
      "| to_tune_59710_00011 | PENDING  |                 |           64 |        2 |        3 |        4 |        2 | 0.0116825   |\n",
      "| to_tune_59710_00012 | PENDING  |                 |            4 |        2 |        2 |        6 |        3 | 0.00848958  |\n",
      "| to_tune_59710_00013 | PENDING  |                 |            2 |        3 |        2 |       11 |        3 | 0.00479028  |\n",
      "| to_tune_59710_00014 | PENDING  |                 |            2 |        3 |        3 |        6 |        3 | 0.000857253 |\n",
      "| to_tune_59710_00015 | PENDING  |                 |           64 |        2 |        2 |       11 |        2 | 0.000442054 |\n",
      "| to_tune_59710_00016 | PENDING  |                 |           32 |        3 |        2 |       14 |        3 | 0.019874    |\n",
      "| to_tune_59710_00017 | PENDING  |                 |            4 |        2 |        3 |        8 |        2 | 0.00167345  |\n",
      "| to_tune_59710_00018 | PENDING  |                 |           32 |        2 |        2 |       15 |        2 | 0.0101252   |\n",
      "| to_tune_59710_00019 | PENDING  |                 |           16 |        2 |        3 |        6 |        3 | 0.00340739  |\n",
      "| to_tune_59710_00006 | ERROR    | 127.0.0.1:33753 |            2 |        3 |        2 |        7 |        2 | 0.0120684   |\n",
      "+---------------------+----------+-----------------+--------------+----------+----------+----------+----------+-------------+\n",
      "Number of errored trials: 1\n",
      "+---------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name          |   # failures | error file                                                                                                                                                                                               |\n",
      "|---------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| to_tune_59710_00006 |            1 | /Users/pavelkiselev/PycharmProjects/pythonProject4/ray_result/to_tune_2022-06-21_04-56-21/to_tune_59710_00006_6_batch_size=2,layer1=3,layer2=2,layer3=7,layer4=2,lr=0.0121_2022-06-21_04-56-24/error.txt |\n",
      "+---------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-21 04:56:33,389\tERROR tune.py:743 -- Trials did not complete: [to_tune_59710_00000, to_tune_59710_00001, to_tune_59710_00002, to_tune_59710_00003, to_tune_59710_00004, to_tune_59710_00005, to_tune_59710_00006, to_tune_59710_00007, to_tune_59710_00008, to_tune_59710_00009, to_tune_59710_00010, to_tune_59710_00011, to_tune_59710_00012, to_tune_59710_00013, to_tune_59710_00014, to_tune_59710_00015, to_tune_59710_00016, to_tune_59710_00017, to_tune_59710_00018, to_tune_59710_00019]\n",
      "2022-06-21 04:56:33,390\tINFO tune.py:747 -- Total run time: 11.46 seconds (11.09 seconds for the tuning loop).\n",
      "2022-06-21 04:56:33,391\tWARNING tune.py:753 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "\u001B[2m\u001B[36m(func pid=33749)\u001B[0m /opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"layer1\": tune.randint(2, 4),\n",
    "    \"layer2\": tune.randint(2, 4),\n",
    "    \"layer3\": tune.randint(4, 16),\n",
    "    \"layer4\": tune.randint(2, 4),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([2, 4, 8, 16, 32, 64])\n",
    "}\n",
    "\n",
    "def to_tune(config, checkpoint_dir=None, data_dir=None):\n",
    "    trainset = datasets.ImageFolder(root=data_dir, transform=tsfm_train)\n",
    "    classes = trainset.classes\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    card = int(len(trainset) * 0.8)\n",
    "    trainset, valset = random_split(trainset, [card, len(trainset) - card])\n",
    "\n",
    "    trainloader = DataLoader(dataset=trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(dataset=valset, batch_size=16, shuffle=True)\n",
    "\n",
    "    trainloader.classes, valloader.classes = classes, classes\n",
    "\n",
    "\n",
    "    model = ResNet(\n",
    "        block_cls=BasicBlock,\n",
    "        layers=[config[\"layer1\"], config[\"layer2\"], config[\"layer3\"], config[\"layer4\"]],\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    callback = Tuner()\n",
    "\n",
    "    torchmodel = TorchModel(model, optimizer, criterion, callback=callback)\n",
    "    torchmodel.train(trainloader, valloader, epochs=100)\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=100,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "reporter = CLIReporter(\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"]\n",
    ")\n",
    "result = tune.run(\n",
    "    partial(to_tune, data_dir=train_dir),\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": int(torch.cuda.is_available())},\n",
    "    config=config,\n",
    "    num_samples=20,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    local_dir=(cwd / \"ray_result\")\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(best_trial.last_result[\"accuracy\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-a1cd70f4",
   "language": "python",
   "display_name": "PyCharm (pythonProject4)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}